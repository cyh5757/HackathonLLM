import logging
import sys
from langchain_postgres import PGVector
from app.api.dto.models import Decision_maker, GraphState
from app.repository import pgvector_repository
from app.services.rag_service_test import SNACK_RAG_PROMPT
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.agents import tool
from app.core.config import settings
from langchain_community.utilities import SQLDatabase
from langchain_community.agent_toolkits import create_sql_agent
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda
from datetime import datetime
from zoneinfo import ZoneInfo
from typing import TypedDict
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables.history import RunnableWithMessageHistory
from operator import itemgetter
from langchain_postgres.vectorstores import DistanceStrategy
import asyncio

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
embeddings = OpenAIEmbeddings(model="text-embedding-3-small", dimensions=1536)

if sys.platform == "win32":
    DB_URI = "postgresql+psycopg://postgres:123123@localhost:5432/test"
else:
    DB_URI = str(settings.SQLALCHEMY_DATABASE_URI)

TABLES = ["snack", "snack_additive"]


def create_agent_executor(
    db_uri: str,
    selected_tables: list[str],
    model_name: str = "gpt-3.5-turbo",
    temperature: float = 0.0,
    verbose: bool = True,
):
    db = SQLDatabase.from_uri(db_uri, include_tables=selected_tables)
    llm = ChatOpenAI(model=model_name, temperature=temperature)
    return create_sql_agent(llm, db=db, agent_type="openai-tools", verbose=verbose)


agent_executor = create_agent_executor(DB_URI, TABLES)

vector_store = PGVector(
    embeddings=embeddings,
    collection_name="my_docs",
    embedding_length=1536,
    distance_strategy=DistanceStrategy.COSINE,
    connection=DB_URI,
    logger=logging.getLogger(__name__),
    create_extension=True,
    pre_delete_collection=False,
    use_jsonb=True,
    async_mode=True,
)


async def query_snack_recommendation(
    agent_executor, user_question: str, limit: int = 5
) -> str:
    prompt = f"""
    ë„ˆëŠ” PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì „ë¬¸ê°€ì•¼.
    ì‚¬ìš©ì ì§ˆë¬¸: '{user_question}'
    ì‚¬ìš©ì ì§ˆë¬¸ì— ë”°ë¼ ì ì ˆí•œ SQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ê³  ê²°ê³¼ë¥¼ ì œê³µí•´. limitëŠ” {limit}ë¡œ ì„¤ì •í•´ì¤˜.
    ë‹¨, 'snack' í…Œì´ë¸”ì—ì„œ 'barcode', 'snack_type', 'name', 'company', 'total_serving_size', 'allergy_list', 'safe_food_mark_list' ì»¬ëŸ¼ë§Œ ì‚¬ìš©í•´.
    """
    result = await agent_executor.ainvoke({"input": prompt})
    return result["output"]


retriever = vector_store.as_retriever(search_kwargs={"k": 5})


async def vector_search(input: str) -> str:
    # 1. ë²¡í„°ë¡œ ì´ˆê¸° ê²€ìƒ‰
    docs: list[Document] = await retriever.ainvoke(input)

    print("\nğŸ§ª [ReRank ì „ ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼]")
    for i, doc in enumerate(docs):
        print(f"{i+1}. {doc.page_content[:100]}...")  # ê¸´ ê²½ìš° ì¼ë¶€ë§Œ

    # 2. ì¬ë­í¬ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    rerank_prompt = PromptTemplate.from_template(
        """
    ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ë²¡í„° ê²€ìƒ‰ì„ í†µí•´ ê²€ìƒ‰ëœ ë¬¸ì„œ ëª©ë¡ì…ë‹ˆë‹¤.
    ë¬¸ì„œë“¤ì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ì—¬ ê´€ë ¨ë„ê°€ ê°€ì¥ ë†’ì€ ìˆœì„œë¡œ ì •ë ¬í•´ ì£¼ì„¸ìš”.
    ë§¨ ìœ„ì— ì˜¬ìˆ˜ë¡ ê´€ë ¨ë„ê°€ ë†’ë‹¤ê³  íŒë‹¨ë©ë‹ˆë‹¤.

    ì‚¬ìš©ì ì§ˆë¬¸:
    {query}

    ê²€ìƒ‰ëœ ë¬¸ì„œë“¤:
    {documents}

    --- ì¶œë ¥ í¬ë§· ---
    ë¬¸ì„œ ë‚´ìš©ë§Œ ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ëœ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
    """
    )

    joined_docs = "\n".join([doc.page_content for doc in docs])
    rerank_chain = rerank_prompt | llm | StrOutputParser()

    reranked_text = await rerank_chain.ainvoke(
        {"query": input, "documents": joined_docs}
    )

    print("\nğŸ§ª [ReRank í›„ ì •ë ¬ ê²°ê³¼]")
    for i, line in enumerate(reranked_text.strip().split("\n")):
        print(f"{i+1}. {line[:100]}...")

    # 3. ìµœì¢… ìƒìœ„ Nê°œ ë¬¸ì„œ ì¶”ì¶œ
    reranked_lines = reranked_text.strip().split("\n")
    top_docs = "\n---\n".join(reranked_lines[:3])
    return top_docs


store = {}
decision_maker_output_parser = JsonOutputParser(pydantic_object=Decision_maker)
format_instructions = decision_maker_output_parser.get_format_instructions()

verifier_prompt = PromptTemplate(
    template="""
        You are an expert who verifies the relevance of retrieved data to a given query and organizes the data for LLM to use as final reference material.
        You'll get two types of reference data.
        Choose the most relevant reference from multiple sources and organize it for LLM to use as final reference material.

        <Output format>: MUST answer in string text {format_instructions}
        
        <Question>: {query} 
        <Retrieved data>: {retrieved_data}
        """,
    input_variables=["query", "retrieved_data"],
    partial_variables={"format_instructions": format_instructions},
)


async def decision_maker(state: GraphState) -> GraphState:
    chain = verifier_prompt | llm | decision_maker_output_parser
    verified = await chain.ainvoke(
        {"query": state["question"], "retrieved_data": state["context"]}
    )
    state["organize_reference"] = verified["reference"]
    return state


CONTEXT_WINDOW_LENGTH = 10


def get_session_history(session_ids):
    if session_ids not in store:
        store[session_ids] = ChatMessageHistory()

    history = store[session_ids]

    # ìµœê·¼ Nê°œë§Œ ë°˜í™˜í•˜ëŠ” ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë¡œì§
    trimmed_history = ChatMessageHistory()
    trimmed_history.messages = history.messages[-CONTEXT_WINDOW_LENGTH:]

    return trimmed_history


async def llm_answer(state: GraphState) -> GraphState:
    prompt = PromptTemplate.from_template(
        """
        You're an helpful assistant who answers questions based on the given context.

        ---Examples---
        #Previous Chat History:
        {chat_history}

        #Question: 
        {question} 

        #Context: 
        {context} 

        #Answer:
        """
    )
    llm_model = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
    chain = prompt | llm_model | StrOutputParser()

    rag_with_history = RunnableWithMessageHistory(
        chain,
        get_session_history,
        input_messages_key="question",
        history_messages_key="chat_history",
    )

    input_data = {
        "question": state["question"],
        "chat_history": itemgetter("chat_history"),
        "context": state["context"],
    }

    response = await rag_with_history.ainvoke(
        input_data, config={"configurable": {"session_id": "rag123"}}
    )

    return GraphState(
        answer=response,
        context=state["context"],
        question=state["question"],
    )


if __name__ == "__main__":
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    async def test_sql_agent():
        print("ğŸ§ª [SQL Agent í…ŒìŠ¤íŠ¸]")
        question = "ì•Œë ˆë¥´ê¸° ìœ ë°œ ì„±ë¶„ì´ ì—†ëŠ” ê°ìì¹©ì„ ì•Œë ¤ì¤˜"
        result = await query_snack_recommendation(agent_executor, question)
        print("ê²°ê³¼:\n", result)

    async def test_vector_search():
        print("\nğŸ§ª [Vector Search í…ŒìŠ¤íŠ¸]")
        query = "ì•„ì´ë“¤ì´ ë¨¹ê¸° ì•ˆì „í•œ ê³¼ì ì•Œë ¤ì¤˜"
        docs: list[Document] = await vector_store.asimilarity_search(query, k=5)
        print("\n".join([doc.page_content for doc in docs]))

    async def test_decision_maker():
        print("\nğŸ§ª [Decision Maker í…ŒìŠ¤íŠ¸]")
        test_state = {
            "question": "ì´ ê³¼ìê°€ ì•ˆì „í•œì§€ ì•Œë ¤ì¤˜",
            "context": {
                "sql": "SQLì—ì„œ ì°¾ì€ ê²°ê³¼ ë¬¸ì„œë“¤...",
                "vector": "ë²¡í„°ì—ì„œ ì°¾ì€ ê²°ê³¼ ë¬¸ì„œë“¤...",
            },
            "organize_reference": "",
        }
        updated = await decision_maker(test_state)
        print("ì„ íƒëœ reference:\n", updated["organize_reference"])

    async def test_llm_answer():
        print("\nğŸ§ª [LLM Answer í…ŒìŠ¤íŠ¸]")
        state = {
            "question": "ì´ ê³¼ìëŠ” ì•Œë ˆë¥´ê¸°ê°€ ì—†ë‚˜ìš”?",
            "context": "ê³¼ì AëŠ” ì•Œë ˆë¥´ê¸° ì„±ë¶„ì´ ì—†ìŠµë‹ˆë‹¤. ì•ˆì „ ë§ˆí¬ë„ ìˆìŠµë‹ˆë‹¤.",
            "organize_reference": "",
        }
        result_state = await llm_answer(state)
        print("ë‹µë³€:\n", result_state["answer"])

    async def main():
        await test_sql_agent()
        await test_vector_search()
        await test_decision_maker()
        await test_llm_answer()

    asyncio.run(main())
